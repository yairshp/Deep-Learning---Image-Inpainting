{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62oH9Ae73yU6"
      },
      "source": [
        "#Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R7X6uPv31uQ"
      },
      "source": [
        "Please mount your drive (in the first block)\n",
        "\n",
        "### How to test a trained model on multiple images\n",
        "\n",
        "1. Go to \"Test on multiple images\" section.\n",
        "2. Fill in the path to the test directory (TEST_PATH variable).\n",
        "  * The script expects the test directory to be in this structue:\n",
        "    * test_dir\n",
        "      * test_set_1\n",
        "      * test_set_2\n",
        "      * test_set_3\n",
        "3. Fill in the path for both the generator of Monet dataset and Photos dataset (GENERATOR_PATH_MONET, GENERATOR_PATH_PHOTOS).\n",
        "4. Fill in (if needed. Right now they are just as the test we were given) the names of the test sets (the test sets refers to the directories in the test directory (test_set_1, test_set_2 and test_set_3 in the example above).\n",
        "5. Run the block of the variables you filled and the block after that (which contains the functions)\n",
        "6. Run the block that calls the \"test\" method. Pass as arguments a test set for your choice, and a boolean value indicating if you are running Monet dataset or Photos dataset (True for Monet, False for Photos).\n",
        "  * When running Monet dataset, you need to run the block which contains the function \"build_generator\" (first block under \"Declare Models\" section).\n",
        "\n",
        "### How to test a traind model on a single image\n",
        "\n",
        "1. Go to \"Test on single image\" section.\n",
        "2. Fill in the path to the image and the mask.\n",
        "3. Fill in the path for both the generator of Monet dataset and Photos dataset (GENERATOR_PATH_MONET, GENERATOR_PATH_PHOTOS).\n",
        "4. Run all the blocks in the section.\n",
        "  * Pass as arguments a test set for your choice, and a boolean value indicating if you are running Monet dataset or Photos dataset (True for Monet, False for Photos).\n",
        "  * When running Monet dataset, you need to run the block which contains the function \"build_generator\" (first block under \"Declare Models\" section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtee2wuUk_mD"
      },
      "source": [
        "# Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq4cY3CpFggo",
        "outputId": "82baa7ec-ccb5-447d-8eb9-0570e204a4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAdCOdJ4Fo90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import imageio\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.initializers import RandomNormal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Input, BatchNormalization, Dropout, Activation, Flatten, Dense, MaxPooling2D, Permute, Reshape, Conv2DTranspose, Lambda\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Model, Sequential, load_model\n",
        "import keras.layers as layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import random\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img, save_img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmViaXUcircg"
      },
      "outputs": [],
      "source": [
        "DATASET_MONET_PATH = '/content/drive/MyDrive/deep learning data/data/monet'\n",
        "DATASET_PHOTOS_PATH = '/content/drive/MyDrive/deep learning data/data/photos'\n",
        "MASK_PATH = '/content/drive/MyDrive/deep learning data/masks'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_MQCjpdk0qr"
      },
      "source": [
        "# Declare Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YztbM1QGXHaQ"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = (256, 256, 3)\n",
        "PATCH_SIZE = 3\n",
        "MASK_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNgpt5S2aEnW"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = (256, 256, 3)\n",
        "\n",
        "def build_generator():\n",
        "  encoder_input = layers.Input(shape=IMG_SHAPE)\n",
        "  c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='conv1')(encoder_input)\n",
        "  c1 = layers.Dropout(0.1)(c1)\n",
        "  c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "  p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "  c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='conv2')(p1)\n",
        "  c2 = layers.Dropout(0.1)(c2)\n",
        "  c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "  p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='conv3')(p2)\n",
        "  c3 = layers.Dropout(0.2)(c3)\n",
        "  c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "  p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "  c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "  c4 = layers.Dropout(0.2)(c4)\n",
        "  c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "  p4 = layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "  c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "  c5 = layers.Dropout(0.3)(c5)\n",
        "  c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "  # Expansive path \n",
        "  u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = layers.concatenate([u6, c4])\n",
        "  c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "  c6 = layers.Dropout(0.2)(c6)\n",
        "  c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "  u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = layers.concatenate([u7, c3])\n",
        "  c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "  c7 = layers.Dropout(0.2)(c7)\n",
        "  c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "  u8 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = layers.concatenate([u8, c2])\n",
        "  c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "  c8 = layers.Dropout(0.1)(c8)\n",
        "  c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "  u9 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = layers.concatenate([u9, c1], axis=3)\n",
        "  c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "  c9 = layers.Dropout(0.1)(c9)\n",
        "  c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "  outputs = layers.Conv2D(3, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "  return Model(inputs=[encoder_input], outputs=[outputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apkj1q4-qDPv"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=IMG_SHAPE, name='input_image')\n",
        "\n",
        "  result = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer, use_bias=False)(inp)\n",
        "  result = LeakyReLU(alpha=0.2)(result)\n",
        "  result = MaxPooling2D((2, 2), padding=\"same\")(result)\n",
        "  result = BatchNormalization(momentum=0.8)(result)\n",
        "\n",
        "  result = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer, use_bias=False)(result)\n",
        "  result = LeakyReLU(alpha=0.2)(result)\n",
        "  result = MaxPooling2D((2, 2), padding=\"same\")(result)\n",
        "  result = BatchNormalization(momentum=0.8)(result)\n",
        "\n",
        "  result = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=initializer, use_bias=False)(result)\n",
        "  result = LeakyReLU(alpha=0.2)(result)\n",
        "  result = MaxPooling2D((2, 2), padding=\"same\")(result)\n",
        "  result = BatchNormalization(momentum=0.8)(result)\n",
        "  \n",
        "  result = tf.keras.layers.ZeroPadding2D()(result) \n",
        "  result = tf.keras.layers.Conv2D(512, (4,4), strides=1, kernel_initializer=initializer, use_bias=False)(result) \n",
        "\n",
        "  result = tf.keras.layers.BatchNormalization()(result)\n",
        "  result = tf.keras.layers.LeakyReLU()(result)\n",
        "                                \n",
        "  patch_out = Activation('sigmoid')(result)\n",
        "\n",
        "  return tf.keras.Model(inputs=inp, outputs=result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-piy6OSkjzl2"
      },
      "outputs": [],
      "source": [
        "def build_combined_generator_photo(generator, discriminator):\n",
        "  discriminator.trainable = False\n",
        "  combined_input = Input(shape=IMG_SHAPE)\n",
        "  generated_img = generator(combined_input)\n",
        "  is_real = discriminator(generated_img)\n",
        "  generated_img = Lambda(K.identity, name='generator_output')(generated_img)\n",
        "  is_real = Lambda(K.identity, name='discriminator_output')(is_real)\n",
        "  return Model(combined_input, [generated_img, is_real])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rv2VA_Ib2-yC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XkcTkmn0iCF"
      },
      "outputs": [],
      "source": [
        "def build_combined_generator_monet(generator, discriminator):\n",
        "  discriminator.trainable = False\n",
        "  combined_input = Input(shape=IMG_SHAPE)\n",
        "  generated_output = generator(combined_input)\n",
        "  is_real = discriminator(generated_output)\n",
        "  generated_img = Lambda(K.identity, name='generator_output')(generated_output)\n",
        "  generated_img2 = Lambda(K.identity, name='generator_output2')(generated_output)\n",
        "  is_real = Lambda(K.identity, name='discriminator_output')(is_real)\n",
        "  return Model(combined_input, [generated_img, generated_img2, is_real])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am1i1O3IopMK"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSyvlN3Ra5qH",
        "outputId": "cf902a10-e2ba-415b-d77b-839a9500d6fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 830 images belonging to 1 classes.\n",
            "Found 207 images belonging to 1 classes.\n",
            "Found 384 images belonging to 1 classes.\n",
            "Found 96 images belonging to 1 classes.\n",
            "Found 60 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "datagen_photos = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_imgs_photos = datagen_photos.flow_from_directory(\n",
        "    DATASET_PHOTOS_PATH,\n",
        "    class_mode=None,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    shuffle=False\n",
        ")\n",
        "val_imgs_photos = datagen_photos.flow_from_directory(\n",
        "    DATASET_PHOTOS_PATH,\n",
        "    class_mode=None,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "datagen_monet = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_imgs_monet = datagen_monet.flow_from_directory(\n",
        "    DATASET_MONET_PATH,\n",
        "    class_mode=None,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_imgs_monet = datagen_monet.flow_from_directory(\n",
        "    DATASET_MONET_PATH,\n",
        "    class_mode=None,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "datagen_mask = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "masks = datagen_mask.flow_from_directory(\n",
        "    MASK_PATH,\n",
        "    class_mode=None,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lv-V_Kaqjba"
      },
      "source": [
        "# Create Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2x5DpeO9X3C"
      },
      "outputs": [],
      "source": [
        "def create_mask_images(images):\n",
        "  mask_images = np.empty_like(images)\n",
        "  for i, image in enumerate(images):\n",
        "    masked_img = image.copy()\n",
        "    x = np.random.randint(0, len(masks))\n",
        "    y = np.random.randint(0, len(masks[x]))\n",
        "    masked_img[masks[x][y] > 0.1] = 0\n",
        "    mask_images[i] = masked_img\n",
        "  return mask_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOAbMb3phCRL"
      },
      "source": [
        "#Style Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKF7A6UxiXJn"
      },
      "outputs": [],
      "source": [
        "STYLE_WEIGHT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSdnit9WhID7"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(x):\n",
        "    x = tf.transpose(x, (2, 0, 1))\n",
        "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "    gram = tf.matmul(features, tf.transpose(features))\n",
        "    return gram\n",
        "\n",
        "\n",
        "def layer_style_loss(style, generator_output):\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(generator_output)\n",
        "    channels = IMG_SHAPE[2]\n",
        "    size = IMG_SHAPE[0] * IMG_SHAPE[1]\n",
        "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "\n",
        "\n",
        "def style_loss(model, style_layer_names):\n",
        "    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "    feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)\n",
        "\n",
        "    def compute_loss(generator_output, style_image):\n",
        "        input_tensor = tf.concat(\n",
        "            [generator_output, style_image], axis=0\n",
        "        )\n",
        "        features = feature_extractor(input_tensor)\n",
        "\n",
        "        loss = tf.zeros(shape=())\n",
        "\n",
        "        for layer_name in style_layer_names:\n",
        "            layer_features = features[layer_name]\n",
        "            style_reference_features = layer_features[1, :, :, :]\n",
        "            combination_features = layer_features[0, :, :, :]\n",
        "            sl = layer_style_loss(style_reference_features, combination_features)\n",
        "            loss += (1 / len(style_layer_names)) * sl\n",
        "        return loss\n",
        "    return compute_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwkHxYXhyTYs"
      },
      "source": [
        "#Compile Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfeLkxjtySwl"
      },
      "outputs": [],
      "source": [
        "def create_model_photo():\n",
        "  generator = build_generator()\n",
        "  generator.compile(loss='mse',\n",
        "                    optimizer='adam')\n",
        "\n",
        "  discriminator_copy = build_discriminator()\n",
        "  discriminator_copy.compile(loss='binary_crossentropy',\n",
        "                            optimizer='adam')\n",
        "\n",
        "  discriminator = discriminator_copy\n",
        "\n",
        "  combined_generator = build_combined_generator_photo(generator, discriminator_copy)\n",
        "  combined_generator.compile(loss=['mse', 'binary_crossentropy'],\n",
        "                            optimizer='adam',\n",
        "                            loss_weights=[0.999, 0.001])\n",
        "  \n",
        "  return generator, discriminator, combined_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn_tu2ZNyktX"
      },
      "outputs": [],
      "source": [
        "def create_model_monet():\n",
        "  generator = build_generator()\n",
        "  style_layer_names = [\n",
        "    'conv1',\n",
        "    'conv2',\n",
        "    'conv3'\n",
        "  ]\n",
        "  generator.compile(optimizer='adam',\n",
        "                    loss=['mse', style_loss(generator, style_layer_names)],\n",
        "                    loss_weights=[1 - STYLE_WEIGHT, STYLE_WEIGHT])\n",
        "\n",
        "  discriminator_copy = build_discriminator()\n",
        "  discriminator_copy.compile(loss='binary_crossentropy',\n",
        "                            optimizer='adam')\n",
        "\n",
        "  discriminator = discriminator_copy\n",
        "\n",
        "  combined_generator = build_combined_generator_monet(generator, discriminator_copy)\n",
        "  combined_generator.compile(loss=['mse', style_loss(generator, style_layer_names), 'binary_crossentropy'],\n",
        "                            optimizer='adam',\n",
        "                            loss_weights=[0.998, 0.001, 0.001])\n",
        "  return generator, discriminator, combined_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw73EVXksPRG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W27xdOExf5CG"
      },
      "outputs": [],
      "source": [
        "def create_models_and_datasets(is_monet):\n",
        "  if is_monet:\n",
        "    generator, discriminator, combined_generator = create_model_monet()\n",
        "    train_imgs = train_imgs_monet\n",
        "    val_imgs = val_imgs_monet\n",
        "  else:\n",
        "    generator, discriminator, combined_generator = create_model_photo()\n",
        "    train_imgs = train_imgs_photos\n",
        "    val_imgs = val_imgs_photos\n",
        "  return  generator, discriminator, combined_generator, train_imgs, val_imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2lrRdR4PMDk"
      },
      "outputs": [],
      "source": [
        "def train(is_monet, generator, discriminator, combined_generator, train_imgs, val_imgs):\n",
        "  epochs = 1 # todo\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  for i in range(epochs + 1):\n",
        "    print('epoch:' + str(i))\n",
        "    idx = random.sample(range(len(train_imgs)), len(train_imgs))\n",
        "    count = 0\n",
        "    batch_train_loss = 0\n",
        "    batch_val_loss = 0\n",
        "\n",
        "    for j in range(len(train_imgs)):\n",
        "      real = np.empty((train_imgs[j].shape[0], PATCH_SIZE, PATCH_SIZE, 1))\n",
        "      real.fill(0.8)\n",
        "      fake = np.empty((train_imgs[j].shape[0], PATCH_SIZE, PATCH_SIZE, 1))\n",
        "      fake.fill(0.2)\n",
        "      masked_imgs = create_mask_images(train_imgs[j])\n",
        "      gen_missing = generator.predict(masked_imgs)\n",
        "      d_loss_real = discriminator.train_on_batch(train_imgs[j], real)\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_missing, fake)\n",
        "      if is_monet:\n",
        "        g_loss = combined_generator.train_on_batch(masked_imgs, [train_imgs[j], train_imgs[j], real])\n",
        "      else:\n",
        "        g_loss = combined_generator.train_on_batch(masked_imgs, [train_imgs[j], real])\n",
        "      batch_train_loss += g_loss[1]\n",
        "      count += 1\n",
        "\n",
        "    for j in range(len(val_imgs)):\n",
        "      real = np.empty((val_imgs[j].shape[0], PATCH_SIZE, PATCH_SIZE, 1))\n",
        "      real.fill(0.8)\n",
        "      fake = np.empty((val_imgs[j].shape[0], PATCH_SIZE, PATCH_SIZE, 1))\n",
        "      fake.fill(0.2)\n",
        "      masked_imgs = create_mask_images(val_imgs[j])\n",
        "      gen_missing = generator.predict(masked_imgs)\n",
        "      if is_monet:\n",
        "        g_loss = combined_generator.test_on_batch(masked_imgs, [val_imgs[j], val_imgs[j], real])\n",
        "      else:\n",
        "        g_loss = combined_generator.test_on_batch(masked_imgs, [val_imgs[j], real])\n",
        "      batch_val_loss += g_loss[1]\n",
        "\n",
        "    print(f'train loss: {batch_train_loss / len(train_imgs)}, val loss: {batch_val_loss / len(val_imgs)}')\n",
        "    train_loss.append(batch_train_loss / len(train_imgs))\n",
        "    val_loss.append(batch_val_loss / len(val_imgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aedQBIIITN5G",
        "outputId": "36e6592d-58b6-49d4-d119-a9c6ae5abda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:0\n",
            "train loss: 0.018658598187771117, val loss: 0.009005992615129799\n",
            "saving\n",
            "epoch:1\n",
            "train loss: 0.008408413861285557, val loss: 0.007097969154446301\n",
            "epoch:2\n",
            "train loss: 0.006260364778096449, val loss: 0.005922251822829077\n",
            "epoch:3\n",
            "train loss: 0.0066670823467905975, val loss: 0.005222031986870041\n",
            "epoch:4\n",
            "train loss: 0.005498258908284532, val loss: 0.0043410395340867\n",
            "epoch:5\n",
            "train loss: 0.004680037852350241, val loss: 0.0039251019638455046\n",
            "saving\n",
            "epoch:6\n",
            "train loss: 0.004395856602059212, val loss: 0.003929231098895384\n",
            "epoch:7\n",
            "train loss: 0.0041147409342970195, val loss: 0.008959797687235881\n",
            "epoch:8\n",
            "train loss: 0.004400331296809864, val loss: 0.0039058241395237433\n",
            "epoch:9\n",
            "train loss: 0.004106013822655024, val loss: 0.0038037268148565836\n",
            "epoch:10\n",
            "train loss: 0.0038585256704457356, val loss: 0.0034055594428950412\n",
            "saving\n",
            "epoch:11\n",
            "train loss: 0.0036464795292968947, val loss: 0.004353094851301814\n",
            "epoch:12\n",
            "train loss: 0.0035869089664298704, val loss: 0.0033186185544102705\n",
            "epoch:13\n",
            "train loss: 0.003443264905813108, val loss: 0.0031899765149732543\n",
            "epoch:14\n",
            "train loss: 0.0034065230141674297, val loss: 0.0032701851636044344\n",
            "epoch:15\n",
            "train loss: 0.0033134016385619443, val loss: 0.003161983735150319\n",
            "saving\n",
            "epoch:16\n",
            "train loss: 0.003177870531296569, val loss: 0.0032097487181255765\n",
            "epoch:17\n",
            "train loss: 0.003119573706499068, val loss: 0.003151548170452853\n",
            "epoch:18\n",
            "train loss: 0.003076575053869006, val loss: 0.0031664874669249084\n",
            "epoch:19\n",
            "train loss: 0.00305458579267989, val loss: 0.0029815344023518264\n",
            "epoch:20\n",
            "train loss: 0.0030690613910502925, val loss: 0.0031745159199503673\n",
            "saving\n",
            "epoch:21\n",
            "train loss: 0.0029776507419063073, val loss: 0.003084267666880888\n",
            "epoch:22\n",
            "train loss: 0.0029541359743399716, val loss: 0.0033228619643275374\n",
            "epoch:23\n",
            "train loss: 0.0029615154622164978, val loss: 0.0030960307459198784\n",
            "epoch:24\n",
            "train loss: 0.002876453898352338, val loss: 0.0033123446152206848\n",
            "epoch:25\n",
            "train loss: 0.002886475419720889, val loss: 0.0033066943975758145\n",
            "saving\n",
            "epoch:26\n",
            "train loss: 0.002866671942849114, val loss: 0.0033480709429237654\n",
            "epoch:27\n",
            "train loss: 0.002842337546594949, val loss: 0.003094512672925537\n",
            "epoch:28\n",
            "train loss: 0.0027970959993333304, val loss: 0.0036454375288237566\n",
            "epoch:29\n",
            "train loss: 0.002822997470172016, val loss: 0.0035553758365991102\n",
            "epoch:30\n",
            "train loss: 0.0028132427198050373, val loss: 0.0036344371328596026\n",
            "saving\n",
            "epoch:31\n",
            "train loss: 0.002794801686170765, val loss: 0.0035701902339827607\n",
            "epoch:32\n",
            "train loss: 0.002725668835575396, val loss: 0.0033601736729244954\n",
            "epoch:33\n",
            "train loss: 0.0027365680679477837, val loss: 0.0034938730294181205\n",
            "epoch:34\n",
            "train loss: 0.002757060494001383, val loss: 0.003434333849740638\n",
            "epoch:35\n",
            "train loss: 0.0027192238002581607, val loss: 0.0036309602120044556\n",
            "saving\n",
            "epoch:36\n",
            "train loss: 0.0026909982757801613, val loss: 0.0036250069732142783\n",
            "epoch:37\n",
            "train loss: 0.0026641482003138994, val loss: 0.003871937753336335\n",
            "epoch:38\n",
            "train loss: 0.0026531145905954127, val loss: 0.0034089007690040903\n",
            "epoch:39\n",
            "train loss: 0.0026315476749335753, val loss: 0.003613679328488863\n",
            "epoch:40\n",
            "train loss: 0.00269101757261193, val loss: 0.0036737174933395263\n",
            "saving\n",
            "epoch:41\n",
            "train loss: 0.002628883248359092, val loss: 0.004014776302607392\n",
            "epoch:42\n",
            "train loss: 0.0026298699571766933, val loss: 0.003547017287928611\n",
            "epoch:43\n",
            "train loss: 0.0026225903582764495, val loss: 0.003983341485515914\n",
            "epoch:44\n",
            "train loss: 0.002611198855447583, val loss: 0.0037134510692505337\n",
            "epoch:45\n",
            "train loss: 0.002543178524898725, val loss: 0.004029580352256413\n",
            "saving\n",
            "epoch:46\n",
            "train loss: 0.0025617921161359514, val loss: 0.0038612435561266134\n",
            "epoch:47\n",
            "train loss: 0.00255205919331109, val loss: 0.0036516183622139083\n",
            "epoch:48\n",
            "train loss: 0.002546931776702298, val loss: 0.0037171682475177063\n",
            "epoch:49\n",
            "train loss: 0.002542447752851083, val loss: 0.003962719683344899\n",
            "epoch:50\n",
            "train loss: 0.002519789162761299, val loss: 0.004094479952155697\n",
            "saving\n",
            "epoch:51\n",
            "train loss: 0.002497638035988943, val loss: 0.003820346215930344\n",
            "epoch:52\n",
            "train loss: 0.002481561887014488, val loss: 0.0040259598119353705\n",
            "epoch:53\n",
            "train loss: 0.0024664890128902202, val loss: 0.0038822448246223344\n",
            "epoch:54\n",
            "train loss: 0.00246378294038798, val loss: 0.0041179100093855095\n",
            "epoch:55\n",
            "train loss: 0.002476695421633353, val loss: 0.003452179723271084\n",
            "saving\n",
            "epoch:56\n",
            "train loss: 0.0024441090460062364, val loss: 0.003993509474887766\n",
            "epoch:57\n",
            "train loss: 0.002445636735170741, val loss: 0.00392433267023245\n",
            "epoch:58\n",
            "train loss: 0.0024114990671726196, val loss: 0.004145135822578926\n",
            "epoch:59\n",
            "train loss: 0.002431817278358556, val loss: 0.004047079707644033\n",
            "epoch:60\n",
            "train loss: 0.002425328338920901, val loss: 0.0037583322370086203\n",
            "saving\n",
            "epoch:61\n",
            "train loss: 0.002402930832348912, val loss: 0.003946359033844518\n",
            "epoch:62\n",
            "train loss: 0.002427282609873642, val loss: 0.004147554427618161\n",
            "epoch:63\n",
            "train loss: 0.0024035052513433834, val loss: 0.0040176501206588\n",
            "epoch:64\n",
            "train loss: 0.002348785354776985, val loss: 0.003943924974671311\n",
            "epoch:65\n",
            "train loss: 0.002387501726347678, val loss: 0.003981914334210821\n",
            "saving\n",
            "epoch:66\n",
            "train loss: 0.002380685647288126, val loss: 0.003930643285540017\n",
            "epoch:67\n",
            "train loss: 0.00234616009600524, val loss: 0.0039576941370879385\n",
            "epoch:68\n",
            "train loss: 0.002341407037337459, val loss: 0.004363601783883165\n",
            "epoch:69\n",
            "train loss: 0.002369194275657223, val loss: 0.004042878829006275\n",
            "epoch:70\n",
            "train loss: 0.0023678845778208183, val loss: 0.004324377837209878\n",
            "saving\n",
            "epoch:71\n",
            "train loss: 0.0023275972764002454, val loss: 0.00424124340696091\n",
            "epoch:72\n",
            "train loss: 0.0023166504809275716, val loss: 0.004382843908388168\n",
            "epoch:73\n",
            "train loss: 0.0023372293170806106, val loss: 0.003920166742649268\n",
            "epoch:74\n",
            "train loss: 0.002320408573723398, val loss: 0.004081949776842852\n",
            "epoch:75\n",
            "train loss: 0.0023502582596186335, val loss: 0.00459160078422759\n",
            "saving\n",
            "epoch:76\n",
            "train loss: 0.0023629737353820183, val loss: 0.004686204581627284\n",
            "epoch:77\n",
            "train loss: 0.0023144174295091284, val loss: 0.004443584200502796\n",
            "epoch:78\n",
            "train loss: 0.0023151784143093128, val loss: 0.004021436677166176\n",
            "epoch:79\n",
            "train loss: 0.0022929354967014992, val loss: 0.004280254385031929\n",
            "epoch:80\n",
            "train loss: 0.0022804814527096987, val loss: 0.0047110618933484975\n",
            "saving\n",
            "epoch:81\n",
            "train loss: 0.002301734314724771, val loss: 0.004279340664983134\n",
            "epoch:82\n",
            "train loss: 0.0022333126491337844, val loss: 0.0043020396101796496\n",
            "epoch:83\n",
            "train loss: 0.0022839901661923664, val loss: 0.004872175002343614\n",
            "epoch:84\n",
            "train loss: 0.002275854329623557, val loss: 0.004408081093887714\n",
            "epoch:85\n",
            "train loss: 0.0022764409527140247, val loss: 0.005229730910452252\n",
            "saving\n",
            "epoch:86\n",
            "train loss: 0.002247429940987124, val loss: 0.005015948004173962\n",
            "epoch:87\n",
            "train loss: 0.0022471026409634346, val loss: 0.004632559771628373\n",
            "epoch:88\n",
            "train loss: 0.0022372047252164603, val loss: 0.004805866597135636\n",
            "epoch:89\n",
            "train loss: 0.0022588324424181005, val loss: 0.004775839833944867\n",
            "epoch:90\n",
            "train loss: 0.002249464028202717, val loss: 0.004790554966100238\n",
            "saving\n",
            "epoch:91\n",
            "train loss: 0.002255358343146244, val loss: 0.005025947003477168\n",
            "epoch:92\n",
            "train loss: 0.002236847693645607, val loss: 0.005202379495709796\n",
            "epoch:93\n",
            "train loss: 0.0022119903868852734, val loss: 0.004742915339937264\n",
            "epoch:94\n",
            "train loss: 0.002236100975013952, val loss: 0.004753741800827397\n",
            "epoch:95\n",
            "train loss: 0.0022161601811438395, val loss: 0.0048272491919554095\n",
            "saving\n",
            "epoch:96\n",
            "train loss: 0.00220829565924677, val loss: 0.004987206635467539\n",
            "epoch:97\n",
            "train loss: 0.0021979835073580034, val loss: 0.0050448489518285814\n",
            "epoch:98\n",
            "train loss: 0.0022116904729045928, val loss: 0.004823914041150023\n",
            "epoch:99\n",
            "train loss: 0.0021814285577089654, val loss: 0.004835884533928369\n",
            "epoch:100\n",
            "train loss: 0.0022151791368246036, val loss: 0.004761344753205776\n",
            "saving\n"
          ]
        }
      ],
      "source": [
        "# is_monet = False\n",
        "is_monet = True\n",
        "\n",
        "generator, discriminator, combined_generator, train_imgs, val_imgs = create_models_and_datasets(is_monet=is_monet)\n",
        "\n",
        "train(is_monet, generator, discriminator, combined_generator, train_imgs, val_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Gs8-rK0g_C"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZaYNOanj918"
      },
      "source": [
        "### Test on multiple images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl4I8zDyjzi7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from keras.preprocessing.image import img_to_array, load_img, save_img\n",
        "from keras.models import load_model\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dcE7dErBBpdq"
      },
      "outputs": [],
      "source": [
        "TEST_PATH = '/content/drive/MyDrive/deep learning data/test' # todo enter test path\n",
        "\n",
        "MONET_BLOCKS = 'monet_blocks'\n",
        "MONET_CENTRAL_BLOCK = 'monet_central_block'\n",
        "PHOTOS_BLOCKS = 'photos_blocks'\n",
        "PHOTOS_CENTRAL_BLOCK = 'photos_central_block'\n",
        "PHOTOS_REGION = 'photos_region'\n",
        "\n",
        "GENERATOR_PATH_MONET = '/content/drive/MyDrive/deep learning/models_final/generator_final_model_2_monet.h5' # todo enter path to generator\n",
        "GENERATOR_PATH_PHOTOS = '/content/drive/MyDrive/deep learning data/models/generator_final_model_1_photos.h5' # todo enter path to generator\n",
        "\n",
        "RESULTS_PATH = '' # todo enter path results directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZbwnpLCzIjIZ"
      },
      "outputs": [],
      "source": [
        "def save_results(images, results_path, mask_type):\n",
        "  for i, img in enumerate(images):\n",
        "    save_img(f'{results_path}/{mask_type}_{i}.jpg', img)\n",
        "\n",
        "def put_generated_parts_on_original_images(originals, generated, masks):\n",
        "  results = np.empty_like(originals)\n",
        "  for i in range(len(originals)):\n",
        "    original_copy = originals[i].copy()\n",
        "    original_copy[masks[i] > 0.95] = generated[i][masks[i] > 0.95]\n",
        "    results[i] = original_copy\n",
        "  return results\n",
        "\n",
        "def turn_mask_to_black(images, masks):\n",
        "  results = np.empty_like(images)\n",
        "  for i, img in enumerate(images):\n",
        "    img_copy = img.copy()\n",
        "    img_copy[masks[i] > 0.95] = 0\n",
        "    results[i] = img_copy\n",
        "  return results\n",
        "\n",
        "def load_test_set_and_masks(test_path):\n",
        "  test_set = {}\n",
        "  masks = {}\n",
        "  for filename in os.listdir(test_path):\n",
        "    if not filename.endswith(\"jpg\"): \n",
        "      continue\n",
        "    img = load_img(f'{test_path}/{filename}')\n",
        "    img = img.resize((256, 256))\n",
        "    if \"mask\" in filename:\n",
        "      masks[filename] = img_to_array(img)\n",
        "    else:\n",
        "      test_set[filename] = img_to_array(img)\n",
        "  test_names = [k for k in test_set.keys()]\n",
        "  test_names.sort()\n",
        "  test_set = [test_set[x] for x in test_names]\n",
        "  masks_names = [k for k in masks.keys()]\n",
        "  masks_names.sort()\n",
        "  masks = [masks[x] for x in masks_names]\n",
        "  return np.array(test_set) / 255, np.array(masks) / 255\n",
        "\n",
        "\n",
        "def test(mask_type, is_monet):\n",
        "  if is_monet:\n",
        "    # generator = load_model(GENERATOR_PATH_MONET)\n",
        "\n",
        "    generator = build_generator()\n",
        "    generator.load_weights(GENERATOR_PATH_MONET)\n",
        "  else:\n",
        "    generator = load_model(GENERATOR_PATH_PHOTOS)\n",
        "\n",
        "  test_set, masks = load_test_set_and_masks(f'{TEST_PATH}/{mask_type}')\n",
        "  test_set = turn_mask_to_black(test_set, masks)\n",
        "  results = generator(test_set)\n",
        "  results = put_generated_parts_on_original_images(test_set, results, masks)\n",
        "  save_results(results, RESULTS_PATH, mask_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NP50-WTRDf9L",
        "outputId": "a853e3e1-58e1-4b53-eb07-2e4d1b892fd3"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5dd15bf240d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHOTOS_CENTRAL_BLOCK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# test(PHOTOS_CENTRAL_BLOCK) todo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-95eaaf4e7a19>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(mask_type, is_style)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGENERATOR_PATH_MONET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGENERATOR_PATH_PHOTOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_test_set_and_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{TEST_PATH}/{mask_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/MyDrive/deep learning/models_final/generator_final_model_2_photos.h5"
          ]
        }
      ],
      "source": [
        "# test(MONET_CENTRAL_BLOCK, True)\n",
        "test(PHOTOS_CENTRAL_BLOCK, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UnB52qDkFqz"
      },
      "source": [
        "### Test on single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iyQMIJQwQ2q"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array, load_img, save_img\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbUaT01akHxJ"
      },
      "outputs": [],
      "source": [
        "GENERATOR_PATH_MONET = '/content/drive/MyDrive/deep learning/models_final/generator_final_model_2_monet.h5' # todo enter path to Monet generator\n",
        "GENERATOR_PATH_PHOTOS = '/content/drive/MyDrive/deep learning data/models/generator_final_model_1_photos.h5' # todo enter path to Photos generator\n",
        "IMAGE_PATH = '/content/drive/MyDrive/photo/photo_jpg/fff5c33050.jpg' # enter path to test image\n",
        "MASK_PATH = '/content/drive/MyDrive/deep learning data/masks/masks/mask3.jpg' # enter path to test mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uo8kTcPlB5T"
      },
      "outputs": [],
      "source": [
        "def load_image_and_mask(image_path, mask_path):\n",
        "  img = load_img(image_path)\n",
        "  img = img.resize((256, 256))\n",
        "  img = img_to_array(img) / 255\n",
        "  mask = load_img(mask_path)\n",
        "  mask = mask.resize((256, 256))\n",
        "  mask = img_to_array(mask) / 255\n",
        "  return img, mask\n",
        "\n",
        "def turn_mask_to_black(image, mask):\n",
        "  img_copy = image.copy()\n",
        "  img_copy[mask > 0.95] = 0\n",
        "  return img_copy\n",
        "\n",
        "def put_generated_parts_on_original(original, generated, mask):\n",
        "  original_copy = original.copy()\n",
        "  original_copy[mask > 0.95] = generated[0][mask > 0.95]\n",
        "  return original_copy\n",
        "\n",
        "def show_result(result):\n",
        "  plt.imshow(result)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzlLmwm_koPi"
      },
      "outputs": [],
      "source": [
        "def test_single_image(image_path, mask_path, is_monet):\n",
        "  if is_monet:\n",
        "    generator = build_generator()\n",
        "    generator.load_weights(GENERATOR_PATH_MONET)\n",
        "  else:\n",
        "    generator = load_model(GENERATOR_PATH_PHOTOS)\n",
        "\n",
        "  img, mask = load_image_and_mask(image_path, mask_path)\n",
        "  img = turn_mask_to_black(img, mask)\n",
        "  result = generator(np.array([img]))\n",
        "  result = put_generated_parts_on_original(img, result, mask)\n",
        "  show_result(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "PEGZu0tImnK5",
        "outputId": "eafcf117-33ec-4ead-b170-b3ddedf82283"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a0d931818e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASK_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# test_single_image(IMAGE_PATH, MASK_PATH, True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-de0ac1721f04>\u001b[0m in \u001b[0;36mtest_single_image\u001b[0;34m(image_path, mask_path, is_monet)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGENERATOR_PATH_MONET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGENERATOR_PATH_PHOTOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_and_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/MyDrive/deep learning data/models/generator_final_model_1_photos.h5"
          ]
        }
      ],
      "source": [
        "test_single_image(IMAGE_PATH, MASK_PATH, False)\n",
        "# test_single_image(IMAGE_PATH, MASK_PATH, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTsGLk4K10yc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Image Inpainting 1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}